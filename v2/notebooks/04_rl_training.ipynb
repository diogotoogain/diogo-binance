{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - RL Training\n",
    "\n",
    "Train Reinforcement Learning agents for trading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "from rl import TradingEnvironment, RLTrainer, RewardFunctions\n",
    "from rl.environment import FlatActionWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = Path('../config/default.yaml')\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"RL Configuration:\")\n",
    "rl_config = config['rl']\n",
    "print(f\"  Observation features: {rl_config['environment']['observation_features']}\")\n",
    "print(f\"  Reward function: {rl_config['environment']['reward_function']}\")\n",
    "print(f\"  Episode length: {rl_config['environment']['episode_length']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or create data\n",
    "SYMBOL = config['market']['symbol']\n",
    "features_path = Path('../data/processed') / f'{SYMBOL}_features.parquet'\n",
    "\n",
    "if features_path.exists():\n",
    "    data = pd.read_parquet(features_path)\n",
    "else:\n",
    "    # Create synthetic data\n",
    "    print(\"Creating synthetic data...\")\n",
    "    n = 10000\n",
    "    np.random.seed(42)\n",
    "    returns = np.random.randn(n) * 0.001\n",
    "    prices = 50000 * np.exp(np.cumsum(returns))\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'open': prices * (1 + np.random.randn(n) * 0.0001),\n",
    "        'high': prices * (1 + np.abs(np.random.randn(n) * 0.0005)),\n",
    "        'low': prices * (1 - np.abs(np.random.randn(n) * 0.0005)),\n",
    "        'close': prices,\n",
    "        'volume': np.random.randint(10, 100, n) * 0.1,\n",
    "        'ofi': np.random.randn(n) * 0.3,\n",
    "        'tfi': np.random.randn(n) * 0.3,\n",
    "        'rsi': 50 + np.random.randn(n) * 15,\n",
    "        'adx': 25 + np.random.randn(n) * 10,\n",
    "        'atr': prices * 0.02,\n",
    "        'regime': np.random.randint(0, 3, n),\n",
    "    }, index=pd.date_range(start='2024-01-01', periods=n, freq='1min'))\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trading environment\n",
    "env = TradingEnvironment(config, data)\n",
    "\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test environment\n",
    "obs, info = env.reset()\n",
    "print(f\"Initial observation shape: {obs.shape}\")\n",
    "print(f\"Initial observation: {obs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take some random actions\n",
    "total_reward = 0\n",
    "for _ in range(100):\n",
    "    action = {\n",
    "        'direction': np.random.randint(0, 3),\n",
    "        'position_size': np.random.randint(0, 5),\n",
    "        'sl_mult': np.random.randint(0, 5),\n",
    "        'tp_mult': np.random.randint(0, 5),\n",
    "    }\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(f\"Total reward after random actions: {total_reward:.4f}\")\n",
    "print(f\"Episode stats: {env.get_episode_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train RL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "try:\n",
    "    from stable_baselines3 import PPO\n",
    "    SB3_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SB3_AVAILABLE = False\n",
    "    print(\"stable-baselines3 not installed. Install with:\")\n",
    "    print(\"  pip install stable-baselines3\")\n",
    "\n",
    "if SB3_AVAILABLE:\n",
    "    # Create wrapped environment\n",
    "    train_env = TradingEnvironment(config, data)\n",
    "    wrapped_env = FlatActionWrapper(train_env)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = RLTrainer(config, train_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SB3_AVAILABLE:\n",
    "    # Train PPO agent (reduced timesteps for demo)\n",
    "    print(\"Training PPO agent...\")\n",
    "    trainer.create_agent('ppo')\n",
    "    \n",
    "    results = trainer.train(\n",
    "        'ppo',\n",
    "        total_timesteps=5000,  # Increase for real training\n",
    "    )\n",
    "    \n",
    "    print(f\"Training results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SB3_AVAILABLE:\n",
    "    # Evaluate trained agent\n",
    "    eval_env = TradingEnvironment(config, data)\n",
    "    eval_results = trainer.evaluate('ppo', eval_env, n_episodes=5)\n",
    "    \n",
    "    print(\"Evaluation results:\")\n",
    "    for k, v in eval_results.items():\n",
    "        print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SB3_AVAILABLE:\n",
    "    # Save model\n",
    "    model_path = Path('../models/rl/ppo_trading')\n",
    "    model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    trainer.save('ppo', model_path)\n",
    "    print(f\"Saved model to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Reward Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different reward functions\n",
    "returns = [0.01, -0.005, 0.02, 0.015, -0.01, 0.008, 0.005, -0.003] * 5\n",
    "\n",
    "print(\"Reward function comparisons:\")\n",
    "print(f\"  Sharpe reward: {RewardFunctions.sharpe_reward(returns):.4f}\")\n",
    "print(f\"  Sortino reward: {RewardFunctions.sortino_reward(returns):.4f}\")\n",
    "print(f\"  P&L reward (100 pnl, 10000 balance): {RewardFunctions.pnl_reward(100, 10000):.4f}\")\n",
    "print(f\"  Risk-adjusted (100 pnl, 5% dd): {RewardFunctions.risk_adjusted_reward(100, 5):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test asymmetric reward\n",
    "print(\"\\nAsymmetric reward (2x loss penalty):\")\n",
    "print(f\"  Win $100: {RewardFunctions.asymmetric_reward(100):.2f}\")\n",
    "print(f\"  Lose $100: {RewardFunctions.asymmetric_reward(-100):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for Colab Training\n",
    "\n",
    "For longer training runs, use Google Colab:\n",
    "\n",
    "1. Upload this notebook to Colab\n",
    "2. Enable GPU runtime\n",
    "3. Install dependencies:\n",
    "   ```\n",
    "   !pip install stable-baselines3 gymnasium\n",
    "   ```\n",
    "4. Train with more timesteps (100k+)\n",
    "5. Download trained model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
